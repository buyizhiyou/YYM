{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Normal\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # 使用cuda:1设备\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 生成示例数据：50000个2048维的向量\n",
    "# data = torch.randn(50000, 2048).to(device)  # 假设数据来自标准正态分布\n",
    "\n",
    "# # 数据加载器\n",
    "# batch_size = 128\n",
    "# dataset = TensorDataset(data)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Normalizing Flow的基础组件\n",
    "# class RealNVP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=512):\n",
    "#         super(RealNVP, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         # 使用简单的MLP作为网络\n",
    "#         self.f = nn.Sequential(\n",
    "#             nn.Linear(input_dim // 2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, input_dim // 2)\n",
    "#         )\n",
    "#         self.g = nn.Sequential(\n",
    "#             nn.Linear(input_dim // 2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, input_dim // 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 划分输入向量为两部分\n",
    "#         x1, x2 = x[:, :self.input_dim // 2], x[:, self.input_dim // 2:]\n",
    "        \n",
    "#         # 计算scale和translation\n",
    "#         s = self.f(x1)\n",
    "#         t = self.g(x1)\n",
    "        \n",
    "#         # 对x2进行仿射变换\n",
    "#         z2 = (x2 - t) * torch.exp(-s)\n",
    "        \n",
    "#         # 返回变换后的向量以及log-determinant的雅可比行列式\n",
    "#         log_det_jacobian = -s.sum(dim=1)\n",
    "#         return torch.cat([x1, z2], dim=1), log_det_jacobian\n",
    "\n",
    "# # 定义训练过程\n",
    "# def train_normalizing_flow(model, dataloader, num_epochs=10, lr=1e-3):\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         for batch_idx, (x,) in enumerate(dataloader):\n",
    "#             x = x.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # 前向传播\n",
    "#             z, log_det_jacobian = model(x)\n",
    "            \n",
    "#             # 使用标准正态分布计算对数似然\n",
    "#             prior = Normal(torch.zeros_like(x), torch.ones_like(x))\n",
    "#             log_prob = prior.log_prob(z).sum(dim=1)\n",
    "\n",
    "#             # 负对数似然\n",
    "#             loss = -(log_prob + log_det_jacobian).mean()\n",
    "\n",
    "#             # 反向传播\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# # 创建模型\n",
    "# model = RealNVP(input_dim=2048).to(device)\n",
    "\n",
    "# # 开始训练\n",
    "# train_normalizing_flow(model, dataloader, num_epochs=1000, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.distributions.normal import Normal\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import numpy as np\n",
    "\n",
    "# # 定义RealNVP模型\n",
    "# class RealNVP(nn.Module):\n",
    "#     def __init__(self, num_dims, num_layers):\n",
    "#         super(RealNVP, self).__init__()\n",
    "#         self.num_dims = num_dims\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         # 构建RealNVP的层\n",
    "#         self.made = nn.ModuleList()\n",
    "#         for _ in range(num_layers):\n",
    "#             self.made.append(self._build_layer())\n",
    "        \n",
    "#     def _build_layer(self):\n",
    "#         # 定义一个RealNVP层\n",
    "#         return nn.Sequential(\n",
    "#             nn.Linear(self.num_dims, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, self.num_dims * 2)  # 输出scale和translation\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         log_det_jacobian = 0\n",
    "#         for layer in self.made:\n",
    "#             x, ldj = self._apply_layer(x, layer)\n",
    "#             log_det_jacobian += ldj\n",
    "#         return x, log_det_jacobian\n",
    "    \n",
    "#     def _apply_layer(self, x, layer):\n",
    "#         # 分为两部分: s和t\n",
    "#         s_t = layer(x)\n",
    "#         s, t = s_t.chunk(2, dim=-1)\n",
    "        \n",
    "#         # Scale和Translation\n",
    "#         z = x * torch.exp(s) + t\n",
    "#         log_det_jacobian = s.sum(dim=-1)\n",
    "#         return z, log_det_jacobian\n",
    "\n",
    "# # 数据生成\n",
    "# num_samples = 50000\n",
    "# num_dims = 2048\n",
    "\n",
    "# # 随机生成一些数据\n",
    "# data = np.random.randn(num_samples, num_dims).astype(np.float32)\n",
    "# data_tensor = torch.tensor(data)\n",
    "\n",
    "# # 使用DataLoader\n",
    "# batch_size = 256\n",
    "# dataset = TensorDataset(data_tensor)\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # 模型、优化器和设备\n",
    "# device = torch.device('cuda:1')\n",
    "# model = RealNVP(num_dims=num_dims, num_layers=6).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # 训练模型\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for data in train_loader:\n",
    "#         x = data[0].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # 正向传播\n",
    "#         z, log_det_jacobian = model(x)\n",
    "        \n",
    "#         # 计算负对数似然\n",
    "#         normal_dist = Normal(0, 1)\n",
    "#         log_prob = normal_dist.log_prob(z).sum(dim=-1)\n",
    "#         loss = -(log_prob + log_det_jacobian).mean()\n",
    "\n",
    "#         # 反向传播\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# # 测试样本\n",
    "# model.eval()\n",
    "# test_samples = np.random.randn(10, num_dims).astype(np.float32)\n",
    "# test_tensor = torch.tensor(test_samples).to(device)\n",
    "\n",
    "# # 计算测试样本的概率密度\n",
    "# with torch.no_grad():\n",
    "#     z, log_det_jacobian = model(test_tensor)\n",
    "#     normal_dist = Normal(0, 1)\n",
    "#     log_prob = normal_dist.log_prob(z).sum(dim=-1)\n",
    "#     log_prob_density = log_prob + log_det_jacobian\n",
    "#     prob_density = torch.exp(log_prob_density)\n",
    "\n",
    "# print(\"Test samples probability density:\", prob_density)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:31<00:00,  2.14it/s]\n",
      " 82%|████████▏ | 161/196 [01:13<00:16,  2.18it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 设置设备为 CUDA:1\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义 RealNVP 模型\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=512, num_layers=8):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 每一层的 scale (s) 和 translation (t) 网络\n",
    "        self.s_layers = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(num_features // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_features // 2)\n",
    "        ) for _ in range(num_layers)])\n",
    "\n",
    "        self.t_layers = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(num_features // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_features // 2)\n",
    "        ) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        \"\"\"\n",
    "        前向或逆向变换。\n",
    "        如果 reverse=True，则执行逆变换；否则执行正向变换。\n",
    "        \"\"\"\n",
    "        x1, x2 = x.chunk(2, dim=1)  # 将输入分为两部分\n",
    "        for i in range(self.num_layers):\n",
    "            s = self.s_layers[i](x1)  # 缩放因子\n",
    "            t = self.t_layers[i](x1)  # 平移因子\n",
    "            if reverse:  # 逆变换\n",
    "                x2 = (x2 - t) * torch.exp(-s)\n",
    "            else:  # 正向变换\n",
    "                x2 = x2 * torch.exp(s) + t\n",
    "            x1, x2 = x2, x1  # 每层交换 x1 和 x2\n",
    "        \n",
    "        return torch.cat([x1, x2], dim=1)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        计算输入 x 的对数概率密度。\n",
    "        \"\"\"\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        log_det = 0  # 初始化对数雅可比行列式\n",
    "        for i in range(self.num_layers):\n",
    "            s = self.s_layers[i](x1)\n",
    "            t = self.t_layers[i](x1)\n",
    "            log_det += torch.sum(s, dim=1)  # 累积对数雅可比行列式\n",
    "            x2 = (x2 - t) * torch.exp(-s)  # 更新 x2\n",
    "            x1, x2 = x2, x1  # 每层交换 x1 和 x2\n",
    "\n",
    "        z = torch.cat([x1, x2], dim=1)  # 最终的 z\n",
    "        # 标准正态分布的对数概率密度\n",
    "        log_prob_z = -0.5 * torch.sum(z ** 2, dim=1) - 0.5 * self.num_features * torch.log(torch.tensor(2 * torch.pi))\n",
    "        return log_prob_z + log_det  # 综合对数概率\n",
    "\n",
    "# 生成数据集\n",
    "def generate_data(num_samples, num_features=2048):\n",
    "    \"\"\"\n",
    "    生成正态分布的数据集。\n",
    "    \"\"\"\n",
    "    return torch.randn(num_samples, num_features).to(device)\n",
    "\n",
    "# 实例化模型并移到设备上\n",
    "num_features = 2048\n",
    "model = RealNVP(num_features).to(device)\n",
    "\n",
    "# 生成训练集和测试集\n",
    "train_data = generate_data(50000, num_features)\n",
    "test_data = generate_data(1000, num_features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据的总数量\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回指定索引的数据\n",
    "        return self.data[idx]\n",
    "\n",
    "# 将数据加载到自定义的 Dataset 中\n",
    "dataset = CustomDataset(train_data)\n",
    "# 创建 DataLoader，配置 batch_size 为 256\n",
    "batch_size = 256\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 使用 Adam 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5,momentum=0.9,weight_decay=0.9)\n",
    "\n",
    "# 归一化???\n",
    "# train_data = (train_data - train_data.mean(dim=0)) / train_data.std(dim=0)\n",
    "# test_data = (test_data - train_data.mean(dim=0)) / train_data.std(dim=0)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for x in tqdm(dataloader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 计算负对数似然作为损失\n",
    "        loss = -torch.mean(model.log_prob(train_data))\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# 测试阶段计算测试集的概率密度\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    log_prob_test = model.log_prob(test_data)  # 计算测试集对数概率\n",
    "    prob_density_test = torch.exp(log_prob_test)  # 概率密度\n",
    "\n",
    "# 打印部分测试集的概率密度结果\n",
    "print(\"Test set probability densities (first 10):\")\n",
    "print(prob_density_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
